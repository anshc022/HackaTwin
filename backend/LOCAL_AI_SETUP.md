# ðŸ¤– Local AI Setup Guide for HackaTwin

## Option 1: Ollama (Recommended - Easy & Fast)

### Quick Setup:
1. **Download Ollama**: https://ollama.com/download
2. **Install and run**: The installer will set everything up
3. **Pull a model**: Open terminal and run:
   ```bash
   ollama pull llama3.2:3b    # 2GB - Fast and good quality
   # OR
   ollama pull phi3:mini      # 2.3GB - Microsoft's model
   # OR  
   ollama pull qwen2:1.5b     # 0.9GB - Very fast, smaller
   ```

4. **Start Ollama**: Usually starts automatically, or run:
   ```bash
   ollama serve
   ```

5. **Test**: Your API will now use local AI automatically!

### Alternative Models:
- `gemma2:2b` - Google's efficient model (1.6GB)
- `llama3.2:1b` - Ultra-fast tiny model (0.7GB)
- `codellama:7b` - Great for code generation (3.8GB)

## Option 2: Use Our Setup Script

Run our automated setup:
```bash
python setup_local_ai.py
```

This will:
- Download and install Ollama
- Help you choose a model
- Test the setup
- Update your configuration

## Option 3: Completely Offline (No Internet After Setup)

1. **Install transformers**:
   ```bash
   pip install transformers torch
   ```

2. **Use offline service**:
   - Replace `ai_service.py` with `ai_service_offline.py`
   - Models download once, then work offline
   - Smaller models: DialoGPT-small (~230MB)

## Option 4: Other Local AI Tools

### LM Studio (GUI Interface):
1. Download: https://lmstudio.ai/
2. Download a model through the GUI
3. Start local server
4. Update `.env`:
   ```
   LOCAL_AI_TYPE=lmstudio
   LOCAL_AI_URL=http://localhost:1234
   ```

### LocalAI (OpenAI-compatible):
1. Install: https://localai.io/
2. Setup with docker or binary
3. Update `.env`:
   ```
   LOCAL_AI_TYPE=localai
   LOCAL_AI_URL=http://localhost:8080
   ```

## Configuration

Update your `.env` file:
```bash
# Choose your setup
LOCAL_AI_TYPE=ollama              # or localai, lmstudio
LOCAL_AI_URL=http://localhost:11434  # Default Ollama port
LOCAL_AI_MODEL=llama3.2:3b       # Your chosen model
AI_FALLBACK=true                  # Use templates if AI fails
```

## Testing

1. **Start your API**:
   ```bash
   python start_server.py
   ```

2. **Test endpoint**:
   ```bash
   curl -X POST http://localhost:8000/answer_question \
     -H "Content-Type: application/json" \
     -d '{"question": "How do I join the hackathon?"}'
   ```

3. **Check the response** - it should be generated by your local AI!

## Troubleshooting

### "Connection refused" errors:
- Make sure Ollama is running: `ollama serve`
- Check the port in your `.env` file
- Try: `ollama list` to see installed models

### "Model not found":
- Pull the model: `ollama pull llama3.2:3b`
- Check model name matches `.env` setting

### Slow responses:
- Try a smaller model: `qwen2:1.5b`
- Reduce max_tokens in requests
- Use GPU if available (CUDA)

## Model Recommendations by Use Case

### **Fast & Light** (Good for development):
- `qwen2:1.5b` (0.9GB) - Very fast
- `llama3.2:1b` (0.7GB) - Ultra-fast

### **Balanced** (Recommended):
- `llama3.2:3b` (2GB) - Good quality, reasonable speed
- `phi3:mini` (2.3GB) - Microsoft, good for coding

### **Higher Quality** (If you have more RAM):
- `llama3.1:8b` (4.7GB) - Better responses
- `codellama:7b` (3.8GB) - Excellent for technical content

## Benefits of Local AI

âœ… **No API costs** - Run unlimited requests  
âœ… **Privacy** - Data stays on your machine  
âœ… **Offline capability** - Works without internet  
âœ… **Fast responses** - No network latency  
âœ… **Customizable** - Use any model you want  

## Performance Tips

- **SSD recommended** for faster model loading
- **8GB+ RAM** for smooth operation
- **GPU support** available with CUDA setup
- **Keep models local** once downloaded
